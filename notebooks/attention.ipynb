{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "\n",
    "# Attention\n",
    "from src.components.attention import SelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57226109",
   "metadata": {},
   "source": [
    "**1. Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa87e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a900ad",
   "metadata": {},
   "source": [
    "**2. Embedding creation**\n",
    "\n",
    "*We create a random tensor to simulate the embeddings input to the attention mechanism.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f248608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4035,  0.3935,  0.5077,  ...,  0.0388,  1.7818,  0.0272],\n",
       "        [ 0.3740,  0.1924,  0.7580,  ...,  0.4056, -1.3121, -1.6064],\n",
       "        [-1.0050,  0.7753,  0.8475,  ...,  1.0398,  1.2283, -1.0327],\n",
       "        ...,\n",
       "        [-1.9996, -0.1667, -0.6915,  ..., -0.5849, -1.2648,  0.2952],\n",
       "        [-1.2510,  2.2274, -0.0350,  ..., -0.6277, -0.8276,  0.6030],\n",
       "        [-1.6712,  1.2298, -1.4076,  ..., -0.5431, -0.2476,  0.8017]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create input (from embeddings)\n",
    "# 2 words / 8 tokens each / embedding size 128\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(x.shape)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83980fc4",
   "metadata": {},
   "source": [
    "**3. Self-Attention Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02372e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 11:17:48.992\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mCausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create attention module\n",
    "attn = SelfAttention(d_model, n_heads, max_seq_len=max_seq_len)\n",
    "\n",
    "# Forward pass\n",
    "out = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c79bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e58b3073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2482,  0.6255,  0.4999,  ...,  0.8962, -0.7809, -0.0728],\n",
       "        [-0.4772,  0.0000,  0.1295,  ...,  1.2026, -0.8311, -0.5821],\n",
       "        [ 0.1638,  0.8480,  0.1075,  ...,  0.9776, -0.5605,  0.2416],\n",
       "        ...,\n",
       "        [-0.5093,  0.0854, -0.0293,  ..., -0.0854, -0.4644,  0.2742],\n",
       "        [-0.0546,  0.7255, -0.3142,  ..., -0.3760, -0.3325,  0.0056],\n",
       "        [-0.3602,  0.3267,  0.0000,  ..., -0.2337, -0.2548,  0.1144]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3eae4c",
   "metadata": {},
   "source": [
    "<pre style=\"font-family: Menlo, Consolas, monospace; font-size: 14px; line-height: 1.35;\">\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Input                                        │\n",
    "│ (batch, seq, d_model)                        │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Linear Projection                            │\n",
    "│ Q, K, V                                      │\n",
    "│ (batch, heads, seq, head_dim)                │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Attention Scores                             │\n",
    "│ Q @ Kᵀ / √d_k                                │\n",
    "│ (batch, heads, seq, seq)                     │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Causal Mask                                  │\n",
    "│ Lower triangular mask                        │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Softmax                                      │\n",
    "│ Attention weights                            │\n",
    "│ (batch, heads, seq, seq)                     │\n",
    "│ Σ weights = 1 (along seq dimension)          │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Weighted Sum                                 │\n",
    "│ Weights @ V                                  │\n",
    "│ (batch, heads, seq, head_dim)                │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Reshape                                      │\n",
    "│ Combine heads                                │\n",
    "│ (batch, seq, d_model)                        │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Output Projection                            │\n",
    "│ Final linear                                 │\n",
    "│ (batch, seq, d_model)                        │\n",
    "└──────────────────────────────────────────────┘\n",
    "                    ↓\n",
    "┌──────────────────────────────────────────────┐\n",
    "│ Output                                       │\n",
    "│ (batch, seq, d_model)                        │\n",
    "└──────────────────────────────────────────────┘\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24f5ed",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
