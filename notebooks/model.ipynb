{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5cad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "\n",
    "# Tokenize\n",
    "from src.components.bpe_tokenizer import BPETokenizer\n",
    "# Transformer Block\n",
    "from src.components.model import GPTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1735c",
   "metadata": {},
   "source": [
    "**1. Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7f8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 300\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "d_ff = 512\n",
    "max_seq_len = 128\n",
    "batch_size = 2\n",
    "seq_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8255b4",
   "metadata": {},
   "source": [
    "**2. Instanciate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84bacbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 13:00:09.954\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.token_embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mTokenEmbedding: vocab=300, dim=128\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.positional_encoding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mSinusoidalPositionalEncoding: max_len=128, dim=128, dropout=0.1\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.967\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.input_embeddings\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[34m\u001b[1mInputEmbeddings: vocab=300, dim=128, max_len=128, dropout=0.1\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.layer_norm\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mLayerNorm: d_model=128, eps=1e-06\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.968\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.layer_norm\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mLayerNorm: d_model=128, eps=1e-06\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.971\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mCausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.972\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.feedforward\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mFeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.transformer_block\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1mTransformerBlock: d_model=128, n_heads=4, d_ff=512\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.layer_norm\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mLayerNorm: d_model=128, eps=1e-06\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.973\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.layer_norm\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mLayerNorm: d_model=128, eps=1e-06\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.974\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.attention\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mCausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.975\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.feedforward\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mFeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.975\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.transformer_block\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m37\u001b[0m - \u001b[34m\u001b[1mTransformerBlock: d_model=128, n_heads=4, d_ff=512\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.components.model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mGPTModel initialized: vocab=300, d_model=128, n_heads=4, n_layers=2, d_ff=512, max_seq_len=128\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.components.model\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mTotal parameters: 434,176\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = GPTModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_layers=n_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=max_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecb086",
   "metadata": {},
   "source": [
    "**3. Train the Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11e23f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 13:00:09.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.components.bpe_tokenizer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mTraining BPE with target vocab size: 300\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.components.bpe_tokenizer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mFound 28 word pieces\u001b[0m\n",
      "\u001b[32m2026-02-03 13:00:09.996\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.components.bpe_tokenizer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[32m\u001b[1m\n",
      "Final vocab size: 302\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Machine learning is fascinating and powerful.\n",
    "Transformers have revolutionized natural language processing.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size=vocab_size)\n",
    "tokenizer.train(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b95ef",
   "metadata": {},
   "source": [
    "**4. Encode the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a5dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266, 277, 282]\n",
      "[298, 299, 97, 114, 110, 259]\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The fox jumps\"\n",
    "sentence2 = \"Machine learning\"\n",
    "\n",
    "encoded1 = tokenizer.encode(sentence1) \n",
    "encoded2 = tokenizer.encode(sentence2)\n",
    "print(encoded1)\n",
    "print(encoded2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea150304",
   "metadata": {},
   "source": [
    "**5. Padding/truncating to fixed length**\n",
    "\n",
    "*When using batching, we need to pad/truncate the sequences to a fixed length.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a35d38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266, 277, 282, 0, 0, 0, 0, 0, 0, 0]\n",
      "[298, 299, 97, 114, 110, 259, 0, 0, 0, 0]\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# 4. Pad to same length (seq_len=10) for batching\n",
    "# Pad with 0 (or use your <|pad|> token)\n",
    "padded1 = encoded1 + [0] * (seq_len - len(encoded1))\n",
    "padded2 = encoded2 + [0] * (seq_len - len(encoded2))\n",
    "\n",
    "# Truncate if too long\n",
    "padded1 = padded1[:seq_len]\n",
    "padded2 = padded2[:seq_len]\n",
    "\n",
    "print(padded1)\n",
    "print(padded2)\n",
    "\n",
    "x = torch.tensor([padded1, padded2])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534daf83",
   "metadata": {},
   "source": [
    "**6. Traning mode**\n",
    "\n",
    "*This is the forward pass of the model to obtain the logits that then will be used in the training.*    \n",
    "*Basically, it is the prev step to the softmax and loss calculation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5c2899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 300])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2323,  0.4477,  0.2641,  ...,  0.0418, -0.2899,  0.1908],\n",
       "         [ 0.0621,  0.3840,  0.3036,  ..., -0.0570, -0.3446, -0.2908],\n",
       "         [ 0.1192,  0.2311,  0.2357,  ..., -0.1601, -0.5479, -0.3426],\n",
       "         ...,\n",
       "         [ 0.3862, -0.0397, -0.2463,  ...,  0.0656, -0.2158, -0.1340],\n",
       "         [ 0.1332, -0.2551, -0.1081,  ...,  0.0121, -0.4112, -0.3136],\n",
       "         [ 0.3712, -0.1096, -0.2335,  ...,  0.1228, -0.3239, -0.0541]],\n",
       "\n",
       "        [[ 0.1119,  0.2297,  0.1429,  ...,  0.0237,  0.4376,  0.1735],\n",
       "         [ 0.0795,  0.2076,  0.1459,  ..., -0.0544, -0.4255,  0.4761],\n",
       "         [ 0.0335,  0.0063, -0.1720,  ...,  0.1461,  0.0365, -0.1258],\n",
       "         ...,\n",
       "         [ 0.1671, -0.0316, -0.1185,  ..., -0.0098, -0.2157, -0.1831],\n",
       "         [ 0.2995, -0.1255, -0.0480,  ...,  0.0048, -0.3992, -0.1822],\n",
       "         [ 0.3188, -0.2018, -0.3893,  ...,  0.0040, -0.3512, -0.1650]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(x)\n",
    "print(logits.shape)  # Should be (batch_size, seq_len, vocab_size)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f357212",
   "metadata": {},
   "source": [
    "**7. Inference**\n",
    "\n",
    "*Take a look to the generate() function, but basically, gets the logist (like before) and applies softmax to get the probabilities of the next token*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22406dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[266, 271, 275, 277]])\n",
      "tensor([[266, 271, 275, 277, 173,  71, 287, 259,  20, 189, 270, 246, 257,  55]])\n",
      "The quick brown fox�G theing\u0014� quic�er7\n"
     ]
    }
   ],
   "source": [
    "# Encode\n",
    "prompt_text = \"The quick brown fox\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text)\n",
    "encoded_prompt_tensor = torch.tensor([encoded_prompt])\n",
    "print(encoded_prompt_tensor)\n",
    "\n",
    "# Generate\n",
    "generated = model.generate(\n",
    "    encoded_prompt_tensor,\n",
    "    max_new_tokens=10,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(generated)\n",
    "\n",
    "# Decode\n",
    "encoded_list = generated[0].tolist()\n",
    "decoded_text = tokenizer.decode(encoded_list)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca691c0",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
