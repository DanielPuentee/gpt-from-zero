{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10fc6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import torch\n",
    "\n",
    "# Tokenize\n",
    "from src.components.bpe_tokenizer import BPETokenizer\n",
    "# Input Embeddings (Just the combination of TokenEmbedding and PositionalEncoding)\n",
    "from src.components.input_embeddings import InputEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a09c8",
   "metadata": {},
   "source": [
    "**1. Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4189665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "vocab_size = 300\n",
    "d_model = 128\n",
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd2154",
   "metadata": {},
   "source": [
    "**2. Train the Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c82c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 11:09:11.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.components.bpe_tokenizer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mTraining BPE with target vocab size: 300\u001b[0m\n",
      "\u001b[32m2026-02-03 11:09:11.425\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.components.bpe_tokenizer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mFound 28 word pieces\u001b[0m\n",
      "\u001b[32m2026-02-03 11:09:11.427\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.components.bpe_tokenizer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[32m\u001b[1m\n",
      "Final vocab size: 302\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Machine learning is fascinating and powerful.\n",
    "Transformers have revolutionized natural language processing.\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size=vocab_size)\n",
    "tokenizer.train(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19493825",
   "metadata": {},
   "source": [
    "**3. Encode the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e61b79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266, 277, 282]\n",
      "[298, 299, 97, 114, 110, 259]\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The fox jumps\"\n",
    "sentence2 = \"Machine learning\"\n",
    "\n",
    "encoded1 = tokenizer.encode(sentence1) \n",
    "encoded2 = tokenizer.encode(sentence2)\n",
    "print(encoded1)\n",
    "print(encoded2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59c699",
   "metadata": {},
   "source": [
    "**4. Padding/truncating to fixed length**\n",
    "\n",
    "*When using batching, we need to pad/truncate the sequences to a fixed length.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1be92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266, 277, 282, 0, 0, 0, 0, 0, 0, 0]\n",
      "[298, 299, 97, 114, 110, 259, 0, 0, 0, 0]\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# 4. Pad to same length (seq_len=10) for batching\n",
    "# Pad with 0 (or use your <|pad|> token)\n",
    "padded1 = encoded1 + [0] * (seq_len - len(encoded1))\n",
    "padded2 = encoded2 + [0] * (seq_len - len(encoded2))\n",
    "\n",
    "# Truncate if too long\n",
    "padded1 = padded1[:seq_len]\n",
    "padded2 = padded2[:seq_len]\n",
    "\n",
    "print(padded1)\n",
    "print(padded2)\n",
    "\n",
    "# x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "x = torch.tensor([padded1, padded2])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d8483",
   "metadata": {},
   "source": [
    "**5. Generate the final embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae3d4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-03 11:09:33.816\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.token_embedding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mTokenEmbedding: vocab=300, dim=128\u001b[0m\n",
      "\u001b[32m2026-02-03 11:09:33.819\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.positional_encoding\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mSinusoidalPositionalEncoding: max_len=512, dim=128, dropout=0.1\u001b[0m\n",
      "\u001b[32m2026-02-03 11:09:33.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.components.input_embeddings\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[34m\u001b[1mInputEmbeddings: vocab=300, dim=128, max_len=512, dropout=0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "emb = InputEmbeddings(vocab_size, d_model, max_seq_len, dropout=0.1)\n",
    "out = emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f4abee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c7d6f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
