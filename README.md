# GPT-Style Model from Scratch

<p align="center">
  <img src="./imgs/model_flow.png" alt="Strands Agent Demo" width="1000"/>
</p>

[![Visual Studio Code](https://custom-icon-badges.demolab.com/badge/Visual%20Studio%20Code-0078d7.svg?logo=vsc&logoColor=white)](#)
[![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=fff)](#)
![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)
[![Anaconda](https://img.shields.io/badge/Anaconda-44A833?logo=anaconda&logoColor=fff)](#)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white)
![GitHub last commit](https://img.shields.io/github/last-commit/DanielPuentee/gpt-from-zero)
![Repo size](https://img.shields.io/github/repo-size/DanielPuentee/gpt-from-zero)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![GitHub Copilot](https://img.shields.io/badge/GitHub%20Copilot-000?logo=githubcopilot&logoColor=fff)](#)
![License](https://img.shields.io/badge/License-MIT-green.svg)

> Author: [Daniel Puente Viejo](https://www.linkedin.com/in/danielpuenteviejo/)

A complete implementation of a GPT-style decoder-only transformer built from scratch in PyTorch. This project includes modular components, training scripts, and interactive notebooks for learning and experimentation.

## ğŸ“ Project Structure

```
transformer/
â”œâ”€â”€ config/                 # Configuration files
â”œâ”€â”€ data/                   # Training data
â”œâ”€â”€ checkpoints/            # Saved model checkpoints
â”œâ”€â”€ logs/                   # Training logs
â”œâ”€â”€ notebooks/              # Educational notebooks of each .py of components/ folder
â””â”€â”€ src/                    # Source code
    â”œâ”€â”€ components/         # Transformer components
    â”œâ”€â”€ train_pretrain.py   # Training script with text files
    â”œâ”€â”€ train_finetune.py   # Fine-tuning script with RLHF data (CSV)
    â”œâ”€â”€ inference.ipynb     # Test the model in a notebook
    â””â”€â”€ inference.py        # Inference script
```

## ğŸ§© Understanding the Components (Bottom-Up Order)

### 1. Basic Building Blocks (Independent)

Start here to understand the fundamentals:

- **[notebooks/bpe_tokenizer.ipynb](notebooks/bpe_tokenizer.ipynb)** â†’ [src/components/bpe_tokenizer.py](src/components/bpe_tokenizer.py)
  - Byte-Pair Encoding tokenizer implementation
  - Converts text to tokens and vice versa

- **[notebooks/token_embedding.ipynb](notebooks/token_embedding.ipynb)** â†’ [src/components/token_embedding.py](src/components/token_embedding.py)
  - Converts token IDs to dense vectors
  - Learnable embedding layer

- **[notebooks/positional_encoding.ipynb](notebooks/positional_encoding.ipynb)** â†’ [src/components/positional_encoding.py](src/components/positional_encoding.py)
  - Adds position information to embeddings
  - Uses sinusoidal encoding

- **[notebooks/layer_norm.ipynb](notebooks/layer_norm.ipynb)** â†’ [src/components/layer_norm.py](src/components/layer_norm.py)
  - Layer normalization for stabilizing training
  - Normalizes across feature dimension

- **[notebooks/attention.ipynb](notebooks/attention.ipynb)** â†’ [src/components/attention.py](src/components/attention.py)
  - Multi-head causal self-attention mechanism
  - Core of the transformer architecture

- **[notebooks/feedforward.ipynb](notebooks/feedforward.ipynb)** â†’ [src/components/feedforward.py](src/components/feedforward.py)
  - Position-wise feed-forward network (MLP)
  - Expands then contracts feature dimension

### 2. Combined Components (Mix of Multiple Files)

These combine the basic blocks:

- **[notebooks/input_embeddings.ipynb](notebooks/input_embeddings.ipynb)** â†’ [src/components/input_embeddings.py](src/components/input_embeddings.py)
  - **Combines:** `token_embedding.py` + `positional_encoding.py`
  - Complete input processing: tokens â†’ embeddings â†’ positional encoding

- **[notebooks/transformer_block.ipynb](notebooks/transformer_block.ipynb)** â†’ [src/components/transformer_block.py](src/components/transformer_block.py)
  - **Combines:** `attention.py` + `feedforward.py` + `layer_norm.py`
  - Single transformer layer with residual connections
  - Architecture: LayerNorm â†’ Attention â†’ Add â†’ LayerNorm â†’ FFN â†’ Add

### 3. Complete Model (Everything Together)

- **[notebooks/model.ipynb](notebooks/model.ipynb)** â†’ [src/components/model.py](src/components/model.py)
  - **Combines:** `input_embeddings.py` + `transformer_block.py` (stacked N times)
  - Full GPT-style decoder-only transformer
  - Includes final layer norm and language modeling head

## ğŸ“ How Training Works (Simple Explanation)

### The Core Idea: Predict the Next Word

The model learns by playing a simple game: **"Given these words, what comes next?"**

#### Example

```
Text: "The cat sat on the mat"

Training examples created automatically:
```

<div align="center">

| **Input (see)** | **Target (guess)** |
|-----------------|-------------------|
| "The" | "cat" |
| "The cat" | "sat" |
| "The cat sat" | "on" |
| "The cat sat on" | "the" |
| ... | ... |

</div>

### The Training Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Get batch of text                   â”‚
â”‚     ["The cat sat..."]                  â”‚
â”‚                                         â”‚
â”‚  2. Convert to numbers (tokens)         â”‚
â”‚     [23, 45, 89, 12, ...]               â”‚
â”‚                                         â”‚
â”‚  3. Model makes prediction              â”‚
â”‚     "sat" â†’ 60% confidence âœ“            â”‚
â”‚                                         â”‚
â”‚  4. Calculate error (loss)              â”‚
â”‚     Loss = how wrong was the guess?     â”‚
â”‚                                         â”‚
â”‚  5. Update weights (backpropagation)    â”‚
â”‚     Adjust millions of numbers slightly â”‚
â”‚                                         â”‚
â”‚  6. Repeat millions of times            â”‚
â”‚     Until predictions improve           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What the Model Learns

<div align="center">

| **After Training** | **Example** |
|-------------------|-------------|
| Grammar | "The cat sat" not "cat The sat" |
| Facts | "Michael Jordan won 6 championships" |
| Context | "basketball" follows "NBA" |
| Style | How to answer questions |

</div>

### Key Points

- **No human labels needed** â†’ The text itself teaches the model
- **Causal masking** â†’ Model can only look at past words, not future
- **Self-supervised** â†’ Creates its own training data from raw text

## ğŸš€ Training Pipeline

### Step 1: Pre-training
**File:** [src/train_pretrain.py](src/train_pretrain.py)

Trains the model on raw text data using causal language modeling (predict next token).

```bash
python src/train_pretrain.py
```

**Uses:**
- Data: [data/data.txt](data/data.txt)
- Config: [config/config.json](config/config.json)
- Saves: `checkpoints/model_epoch{N}.pt`

### Step 2: Fine-tuning (Optional)
**File:** [src/train_finetune.py](src/train_finetune.py)

Fine-tunes the pre-trained model on prompt-response pairs with ratings (1-5).

```bash
python src/train_finetune.py
```

**Uses:**
- Data: [data/fine_tune_data.csv](data/fine_tune_data.csv) (columns: prompt, response, rating)
- Pre-trained checkpoint: [checkpoints/model_epoch5.pt](checkpoints/model_epoch5.pt)
- Saves: `checkpoints/model_finetuned_epoch{N}.pt`

### Step 3: Inference
**File:** [src/inference.py](src/inference.py) or [src/inference.ipynb](src/inference.ipynb)

Generate text using the trained model.

```bash
python src/inference.py
```

## ğŸ“š Learning Path

Follow this order to understand the transformer architecture:
0. **Tokenizer** (BPE Tokenizer)
    - Learn how text is converted to tokens

1. **Start with basics:**
   - Token Embedding â†’ Positional Encoding â†’ Layer Norm
   
2. **Learn the core mechanisms:**
   - Attention (how tokens interact)
   - Feedforward (processing each position)
   
3. **See how they combine:**
   - Input Embeddings (token + position)
   - Transformer Block (attention + FFN)
   
4. **Understand the full architecture:**
   - Model (stack of transformer blocks)
   
5. **Run the system:**
   - Training scripts â†’ Inference

## âš™ï¸ Configuration

Edit [config/config.json](config/config.json) to adjust:

```json
{
  "model": {
    "vocab_size": 1000,      // Vocabulary size
    "d_model": 128,          // Embedding dimension
    "n_heads": 4,            // Number of attention heads
    "n_layers": 4,           // Number of transformer blocks
    "d_ff": 512,             // Feedforward hidden dimension
    "max_seq_len": 64,       // Maximum sequence length (window size)
    "dropout": 0.1           // Dropout rate
  },
  "training": {
    "batch_size": 8,
    "learning_rate": 0.0003,
    "n_epochs": 5,
    "device": "cpu"
  }
}
```

## ğŸ“Š Files That Combine Multiple Components

Quick reference for which files are combinations:

- **input_embeddings.py** = token_embedding.py + positional_encoding.py
- **transformer_block.py** = attention.py + feedforward.py + layer_norm.py
- **model.py** = input_embeddings.py + transformer_block.py (Ã—N) + final layer norm

## ğŸ¯ Key Features

- âœ… Built entirely from scratch (no high-level transformer libraries)
- âœ… Modular design with clear separation of components
- âœ… Educational notebooks for each component
- âœ… Complete training pipeline (pre-training + fine-tuning)
- âœ… BPE tokenizer implementation
- âœ… Causal self-attention for autoregressive generation
- âœ… Pre-LayerNorm architecture for training stability
- âœ… Weight tying between embeddings and output head

## ğŸ› ï¸ Requirements

- Python 3.13.5
- PyTorch
- pandas (for fine-tuning)
- loguru (logging)
- tqdm (progress bars)

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

## ğŸ“ Notes

- The model uses a decoder-only (GPT-style) architecture
- Causal masking ensures autoregressive generation
- Checkpoints include model weights, optimizer state, and config
- Tokenizer is trained and saved separately