2026-02-03 13:24:23.568 | INFO     | __main__:train:91 - Using device: cpu
2026-02-03 13:24:23.571 | INFO     | __main__:train:99 - Training new tokenizer
2026-02-03 13:24:23.579 | INFO     | components.bpe_tokenizer:train:63 - Training BPE with target vocab size: 1000
2026-02-03 13:24:23.584 | INFO     | components.bpe_tokenizer:train:67 - Found 6091 word pieces
2026-02-03 13:24:25.746 | SUCCESS  | components.bpe_tokenizer:train:106 - 
Final vocab size: 1002
2026-02-03 13:24:25.753 | INFO     | __main__:train:106 - Tokenizer saved to checkpoints/tokenizer.json
2026-02-03 13:24:25.753 | INFO     | __main__:__init__:32 - Loading data from data/data.txt
2026-02-03 13:24:26.724 | INFO     | __main__:__init__:38 - Total tokens: 12236
2026-02-03 13:24:26.724 | INFO     | __main__:__init__:43 - Number of sequences: 12172
2026-02-03 13:24:26.731 | DEBUG    | components.token_embedding:__init__:24 - TokenEmbedding: vocab=1000, dim=128
2026-02-03 13:24:26.736 | DEBUG    | components.positional_encoding:__init__:54 - SinusoidalPositionalEncoding: max_len=64, dim=128, dropout=0.1
2026-02-03 13:24:26.736 | DEBUG    | components.input_embeddings:__init__:31 - InputEmbeddings: vocab=1000, dim=128, max_len=64, dropout=0.1
2026-02-03 13:24:26.737 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.737 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.738 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 13:24:26.739 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 13:24:26.739 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 13:24:26.740 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.740 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.740 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 13:24:26.741 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 13:24:26.741 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 13:24:26.741 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.741 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.742 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 13:24:26.743 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 13:24:26.743 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 13:24:26.743 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.743 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 13:24:26.743 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 13:24:26.744 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 13:24:26.744 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 13:24:26.747 | INFO     | components.model:__init__:58 - GPTModel initialized: vocab=1000, d_model=128, n_heads=4, n_layers=4, d_ff=512, max_seq_len=64
2026-02-03 13:24:26.747 | INFO     | components.model:__init__:64 - Total parameters: 919,296
2026-02-03 13:24:33.252 | INFO     | __main__:train:133 - Starting training...
2026-02-03 13:24:36.939 | INFO     | __main__:train:172 - Step 100, Loss: 6.2713
2026-02-03 13:24:40.243 | INFO     | __main__:train:172 - Step 200, Loss: 6.1328
2026-02-03 13:24:43.492 | INFO     | __main__:train:172 - Step 300, Loss: 6.0176
2026-02-03 13:24:46.766 | INFO     | __main__:train:172 - Step 400, Loss: 5.8763
2026-02-03 13:24:50.045 | INFO     | __main__:train:172 - Step 500, Loss: 5.7193
2026-02-03 13:24:53.530 | INFO     | __main__:train:172 - Step 600, Loss: 5.5639
2026-02-03 13:24:56.785 | INFO     | __main__:train:172 - Step 700, Loss: 5.4160
2026-02-03 13:25:00.191 | INFO     | __main__:train:172 - Step 800, Loss: 5.2759
2026-02-03 13:25:03.596 | INFO     | __main__:train:172 - Step 900, Loss: 5.1451
2026-02-03 13:25:06.940 | INFO     | __main__:train:172 - Step 1000, Loss: 5.0225
2026-02-03 13:25:10.337 | INFO     | __main__:train:172 - Step 1100, Loss: 4.9074
2026-02-03 13:25:13.645 | INFO     | __main__:train:172 - Step 1200, Loss: 4.7987
2026-02-03 13:25:16.977 | INFO     | __main__:train:172 - Step 1300, Loss: 4.6969
2026-02-03 13:25:20.320 | INFO     | __main__:train:172 - Step 1400, Loss: 4.6008
2026-02-03 13:25:23.994 | INFO     | __main__:train:172 - Step 1500, Loss: 4.5089
2026-02-03 13:25:24.716 | INFO     | __main__:train:176 - Epoch 1 completed. Average loss: 4.4899
2026-02-03 13:25:24.852 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch1.pt
2026-02-03 13:25:27.408 | INFO     | __main__:train:172 - Step 1600, Loss: 3.0805
2026-02-03 13:25:30.665 | INFO     | __main__:train:172 - Step 1700, Loss: 3.0263
2026-02-03 13:25:34.553 | INFO     | __main__:train:172 - Step 1800, Loss: 2.9736
2026-02-03 13:25:37.912 | INFO     | __main__:train:172 - Step 1900, Loss: 2.9142
2026-02-03 13:25:41.311 | INFO     | __main__:train:172 - Step 2000, Loss: 2.8605
2026-02-03 13:25:44.733 | INFO     | __main__:train:172 - Step 2100, Loss: 2.8074
2026-02-03 13:25:48.453 | INFO     | __main__:train:172 - Step 2200, Loss: 2.7560
2026-02-03 13:25:52.100 | INFO     | __main__:train:172 - Step 2300, Loss: 2.7053
2026-02-03 13:25:55.423 | INFO     | __main__:train:172 - Step 2400, Loss: 2.6535
2026-02-03 13:25:58.666 | INFO     | __main__:train:172 - Step 2500, Loss: 2.6023
2026-02-03 13:26:01.989 | INFO     | __main__:train:172 - Step 2600, Loss: 2.5546
2026-02-03 13:26:05.333 | INFO     | __main__:train:172 - Step 2700, Loss: 2.5058
2026-02-03 13:26:08.662 | INFO     | __main__:train:172 - Step 2800, Loss: 2.4594
2026-02-03 13:26:12.017 | INFO     | __main__:train:172 - Step 2900, Loss: 2.4115
2026-02-03 13:26:15.445 | INFO     | __main__:train:172 - Step 3000, Loss: 2.3662
2026-02-03 13:26:16.898 | INFO     | __main__:train:176 - Epoch 2 completed. Average loss: 2.3466
2026-02-03 13:26:16.915 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch2.pt
2026-02-03 13:26:18.796 | INFO     | __main__:train:172 - Step 3100, Loss: 1.6098
2026-02-03 13:26:22.236 | INFO     | __main__:train:172 - Step 3200, Loss: 1.5851
2026-02-03 13:26:25.765 | INFO     | __main__:train:172 - Step 3300, Loss: 1.5514
2026-02-03 13:26:29.600 | INFO     | __main__:train:172 - Step 3400, Loss: 1.5180
2026-02-03 13:26:33.444 | INFO     | __main__:train:172 - Step 3500, Loss: 1.4844
2026-02-03 13:26:37.304 | INFO     | __main__:train:172 - Step 3600, Loss: 1.4527
2026-02-03 13:26:41.088 | INFO     | __main__:train:172 - Step 3700, Loss: 1.4226
2026-02-03 13:26:44.987 | INFO     | __main__:train:172 - Step 3800, Loss: 1.3948
2026-02-03 13:26:48.587 | INFO     | __main__:train:172 - Step 3900, Loss: 1.3662
2026-02-03 13:26:52.074 | INFO     | __main__:train:172 - Step 4000, Loss: 1.3382
2026-02-03 13:26:55.592 | INFO     | __main__:train:172 - Step 4100, Loss: 1.3113
2026-02-03 13:26:59.103 | INFO     | __main__:train:172 - Step 4200, Loss: 1.2847
2026-02-03 13:27:02.755 | INFO     | __main__:train:172 - Step 4300, Loss: 1.2598
2026-02-03 13:27:06.300 | INFO     | __main__:train:172 - Step 4400, Loss: 1.2355
2026-02-03 13:27:09.783 | INFO     | __main__:train:172 - Step 4500, Loss: 1.2122
2026-02-03 13:27:12.101 | INFO     | __main__:train:176 - Epoch 3 completed. Average loss: 1.1972
2026-02-03 13:27:12.118 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch3.pt
2026-02-03 13:27:13.386 | INFO     | __main__:train:172 - Step 4600, Loss: 0.8296
2026-02-03 13:27:17.040 | INFO     | __main__:train:172 - Step 4700, Loss: 0.8241
2026-02-03 13:27:20.652 | INFO     | __main__:train:172 - Step 4800, Loss: 0.8098
2026-02-03 13:27:24.264 | INFO     | __main__:train:172 - Step 4900, Loss: 0.7968
2026-02-03 13:27:27.852 | INFO     | __main__:train:172 - Step 5000, Loss: 0.7866
2026-02-03 13:27:31.372 | INFO     | __main__:train:172 - Step 5100, Loss: 0.7717
2026-02-03 13:27:34.871 | INFO     | __main__:train:172 - Step 5200, Loss: 0.7601
2026-02-03 13:27:38.536 | INFO     | __main__:train:172 - Step 5300, Loss: 0.7487
2026-02-03 13:27:42.105 | INFO     | __main__:train:172 - Step 5400, Loss: 0.7374
2026-02-03 13:27:45.832 | INFO     | __main__:train:172 - Step 5500, Loss: 0.7273
2026-02-03 13:27:49.704 | INFO     | __main__:train:172 - Step 5600, Loss: 0.7176
2026-02-03 13:27:53.271 | INFO     | __main__:train:172 - Step 5700, Loss: 0.7078
2026-02-03 13:27:56.948 | INFO     | __main__:train:172 - Step 5800, Loss: 0.6981
2026-02-03 13:28:00.468 | INFO     | __main__:train:172 - Step 5900, Loss: 0.6890
2026-02-03 13:28:03.884 | INFO     | __main__:train:172 - Step 6000, Loss: 0.6799
2026-02-03 13:28:07.026 | INFO     | __main__:train:176 - Epoch 4 completed. Average loss: 0.6725
2026-02-03 13:28:07.044 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch4.pt
2026-02-03 13:28:07.472 | INFO     | __main__:train:172 - Step 6100, Loss: 0.5310
2026-02-03 13:28:11.150 | INFO     | __main__:train:172 - Step 6200, Loss: 0.5289
2026-02-03 13:28:14.890 | INFO     | __main__:train:172 - Step 6300, Loss: 0.5228
2026-02-03 13:28:18.414 | INFO     | __main__:train:172 - Step 6400, Loss: 0.5163
2026-02-03 13:28:22.016 | INFO     | __main__:train:172 - Step 6500, Loss: 0.5119
2026-02-03 13:28:25.495 | INFO     | __main__:train:172 - Step 6600, Loss: 0.5068
2026-02-03 13:28:28.932 | INFO     | __main__:train:172 - Step 6700, Loss: 0.5017
2026-02-03 13:28:32.386 | INFO     | __main__:train:172 - Step 6800, Loss: 0.4983
2026-02-03 13:28:35.837 | INFO     | __main__:train:172 - Step 6900, Loss: 0.4935
2026-02-03 13:28:39.328 | INFO     | __main__:train:172 - Step 7000, Loss: 0.4891
2026-02-03 13:28:42.902 | INFO     | __main__:train:172 - Step 7100, Loss: 0.4846
2026-02-03 13:28:46.403 | INFO     | __main__:train:172 - Step 7200, Loss: 0.4802
2026-02-03 13:28:49.818 | INFO     | __main__:train:172 - Step 7300, Loss: 0.4765
2026-02-03 13:28:53.288 | INFO     | __main__:train:172 - Step 7400, Loss: 0.4721
2026-02-03 13:28:56.786 | INFO     | __main__:train:172 - Step 7500, Loss: 0.4679
2026-02-03 13:29:00.279 | INFO     | __main__:train:172 - Step 7600, Loss: 0.4640
2026-02-03 13:29:00.628 | INFO     | __main__:train:176 - Epoch 5 completed. Average loss: 0.4635
2026-02-03 13:29:00.644 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch5.pt
2026-02-03 13:29:00.644 | INFO     | __main__:train:199 - Training completed!
2026-02-03 14:49:22.498 | INFO     | __main__:train:91 - Using device: cpu
2026-02-03 14:49:22.499 | INFO     | __main__:train:99 - Training new tokenizer
2026-02-03 14:49:22.502 | INFO     | components.bpe_tokenizer:train:63 - Training BPE with target vocab size: 1000
2026-02-03 14:49:22.504 | INFO     | components.bpe_tokenizer:train:67 - Found 6091 word pieces
2026-02-03 14:49:24.658 | SUCCESS  | components.bpe_tokenizer:train:106 - 
Final vocab size: 1000
2026-02-03 14:49:24.662 | INFO     | __main__:train:106 - Tokenizer saved to checkpoints/tokenizer.json
2026-02-03 14:49:24.662 | INFO     | __main__:__init__:32 - Loading data from data/data.txt
2026-02-03 14:49:25.558 | INFO     | __main__:__init__:38 - Total tokens: 12236
2026-02-03 14:49:25.558 | INFO     | __main__:__init__:43 - Number of sequences: 12172
2026-02-03 14:49:25.561 | DEBUG    | components.token_embedding:__init__:24 - TokenEmbedding: vocab=1000, dim=128
2026-02-03 14:49:25.561 | DEBUG    | components.positional_encoding:__init__:54 - SinusoidalPositionalEncoding: max_len=64, dim=128, dropout=0.1
2026-02-03 14:49:25.561 | DEBUG    | components.input_embeddings:__init__:31 - InputEmbeddings: vocab=1000, dim=128, max_len=64, dropout=0.1
2026-02-03 14:49:25.561 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.561 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.562 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 14:49:25.562 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 14:49:25.562 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 14:49:25.562 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.563 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.563 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 14:49:25.563 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 14:49:25.563 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 14:49:25.563 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.564 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.564 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 14:49:25.565 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 14:49:25.565 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 14:49:25.565 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.565 | DEBUG    | components.layer_norm:__init__:24 - LayerNorm: d_model=128, eps=1e-06
2026-02-03 14:49:25.565 | DEBUG    | components.attention:__init__:54 - CausalSelfAttention: d_model=128, n_heads=4, head_dim=32, dropout=0.1
2026-02-03 14:49:25.565 | DEBUG    | components.feedforward:__init__:44 - FeedForward: d_model=128, d_ff=512, activation=gelu, dropout=0.1
2026-02-03 14:49:25.565 | DEBUG    | components.transformer_block:__init__:37 - TransformerBlock: d_model=128, n_heads=4, d_ff=512
2026-02-03 14:49:25.567 | INFO     | components.model:__init__:58 - GPTModel initialized: vocab=1000, d_model=128, n_heads=4, n_layers=4, d_ff=512, max_seq_len=64
2026-02-03 14:49:25.567 | INFO     | components.model:__init__:64 - Total parameters: 919,296
2026-02-03 14:49:26.401 | INFO     | __main__:train:133 - Starting training...
2026-02-03 14:49:29.946 | INFO     | __main__:train:172 - Step 100, Loss: 6.2742
2026-02-03 14:49:33.301 | INFO     | __main__:train:172 - Step 200, Loss: 6.1296
2026-02-03 14:49:36.762 | INFO     | __main__:train:172 - Step 300, Loss: 6.0205
2026-02-03 14:49:40.138 | INFO     | __main__:train:172 - Step 400, Loss: 5.8755
2026-02-03 14:49:43.855 | INFO     | __main__:train:172 - Step 500, Loss: 5.7129
2026-02-03 14:49:47.332 | INFO     | __main__:train:172 - Step 600, Loss: 5.5524
2026-02-03 14:49:50.809 | INFO     | __main__:train:172 - Step 700, Loss: 5.3990
2026-02-03 14:49:54.310 | INFO     | __main__:train:172 - Step 800, Loss: 5.2564
2026-02-03 14:49:57.711 | INFO     | __main__:train:172 - Step 900, Loss: 5.1234
2026-02-03 14:50:01.301 | INFO     | __main__:train:172 - Step 1000, Loss: 4.9991
2026-02-03 14:50:04.806 | INFO     | __main__:train:172 - Step 1100, Loss: 4.8838
2026-02-03 14:50:08.529 | INFO     | __main__:train:172 - Step 1200, Loss: 4.7753
2026-02-03 14:50:12.018 | INFO     | __main__:train:172 - Step 1300, Loss: 4.6737
2026-02-03 14:50:15.731 | INFO     | __main__:train:172 - Step 1400, Loss: 4.5769
2026-02-03 14:50:19.227 | INFO     | __main__:train:172 - Step 1500, Loss: 4.4848
2026-02-03 14:50:20.008 | INFO     | __main__:train:176 - Epoch 1 completed. Average loss: 4.4650
2026-02-03 14:50:20.034 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch1.pt
2026-02-03 14:50:22.819 | INFO     | __main__:train:172 - Step 1600, Loss: 3.0546
2026-02-03 14:50:26.345 | INFO     | __main__:train:172 - Step 1700, Loss: 3.0021
2026-02-03 14:50:29.935 | INFO     | __main__:train:172 - Step 1800, Loss: 2.9375
2026-02-03 14:50:33.411 | INFO     | __main__:train:172 - Step 1900, Loss: 2.8891
2026-02-03 14:50:37.075 | INFO     | __main__:train:172 - Step 2000, Loss: 2.8361
2026-02-03 14:50:40.680 | INFO     | __main__:train:172 - Step 2100, Loss: 2.7809
2026-02-03 14:50:44.296 | INFO     | __main__:train:172 - Step 2200, Loss: 2.7271
2026-02-03 14:50:47.791 | INFO     | __main__:train:172 - Step 2300, Loss: 2.6765
2026-02-03 14:50:51.254 | INFO     | __main__:train:172 - Step 2400, Loss: 2.6252
2026-02-03 14:50:54.857 | INFO     | __main__:train:172 - Step 2500, Loss: 2.5761
2026-02-03 14:50:58.480 | INFO     | __main__:train:172 - Step 2600, Loss: 2.5269
2026-02-03 14:51:02.038 | INFO     | __main__:train:172 - Step 2700, Loss: 2.4775
2026-02-03 14:51:05.507 | INFO     | __main__:train:172 - Step 2800, Loss: 2.4305
2026-02-03 14:51:09.171 | INFO     | __main__:train:172 - Step 2900, Loss: 2.3844
2026-02-03 14:51:12.933 | INFO     | __main__:train:172 - Step 3000, Loss: 2.3394
2026-02-03 14:51:14.586 | INFO     | __main__:train:176 - Epoch 2 completed. Average loss: 2.3195
2026-02-03 14:51:14.600 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch2.pt
2026-02-03 14:51:16.825 | INFO     | __main__:train:172 - Step 3100, Loss: 1.6027
2026-02-03 14:51:20.554 | INFO     | __main__:train:172 - Step 3200, Loss: 1.5718
2026-02-03 14:51:24.168 | INFO     | __main__:train:172 - Step 3300, Loss: 1.5397
2026-02-03 14:51:27.606 | INFO     | __main__:train:172 - Step 3400, Loss: 1.5059
2026-02-03 14:51:31.026 | INFO     | __main__:train:172 - Step 3500, Loss: 1.4729
2026-02-03 14:51:34.438 | INFO     | __main__:train:172 - Step 3600, Loss: 1.4410
2026-02-03 14:51:37.864 | INFO     | __main__:train:172 - Step 3700, Loss: 1.4102
2026-02-03 14:51:41.374 | INFO     | __main__:train:172 - Step 3800, Loss: 1.3816
2026-02-03 14:51:44.926 | INFO     | __main__:train:172 - Step 3900, Loss: 1.3529
2026-02-03 14:51:48.540 | INFO     | __main__:train:172 - Step 4000, Loss: 1.3267
2026-02-03 14:51:52.245 | INFO     | __main__:train:172 - Step 4100, Loss: 1.3008
2026-02-03 14:51:55.786 | INFO     | __main__:train:172 - Step 4200, Loss: 1.2750
2026-02-03 14:51:59.246 | INFO     | __main__:train:172 - Step 4300, Loss: 1.2500
2026-02-03 14:52:03.142 | INFO     | __main__:train:172 - Step 4400, Loss: 1.2261
2026-02-03 14:52:06.805 | INFO     | __main__:train:172 - Step 4500, Loss: 1.2032
2026-02-03 14:52:09.058 | INFO     | __main__:train:176 - Epoch 3 completed. Average loss: 1.1886
2026-02-03 14:52:09.071 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch3.pt
2026-02-03 14:52:10.251 | INFO     | __main__:train:172 - Step 4600, Loss: 0.8269
2026-02-03 14:52:13.623 | INFO     | __main__:train:172 - Step 4700, Loss: 0.8184
2026-02-03 14:52:17.183 | INFO     | __main__:train:172 - Step 4800, Loss: 0.8044
2026-02-03 14:52:20.676 | INFO     | __main__:train:172 - Step 4900, Loss: 0.7911
2026-02-03 14:52:24.070 | INFO     | __main__:train:172 - Step 5000, Loss: 0.7774
2026-02-03 14:52:27.571 | INFO     | __main__:train:172 - Step 5100, Loss: 0.7664
2026-02-03 14:52:30.936 | INFO     | __main__:train:172 - Step 5200, Loss: 0.7560
2026-02-03 14:52:34.319 | INFO     | __main__:train:172 - Step 5300, Loss: 0.7440
2026-02-03 14:52:37.704 | INFO     | __main__:train:172 - Step 5400, Loss: 0.7337
2026-02-03 14:52:41.136 | INFO     | __main__:train:172 - Step 5500, Loss: 0.7231
2026-02-03 14:52:44.634 | INFO     | __main__:train:172 - Step 5600, Loss: 0.7127
2026-02-03 14:52:48.340 | INFO     | __main__:train:172 - Step 5700, Loss: 0.7034
2026-02-03 14:52:52.105 | INFO     | __main__:train:172 - Step 5800, Loss: 0.6945
2026-02-03 14:52:55.563 | INFO     | __main__:train:172 - Step 5900, Loss: 0.6858
2026-02-03 14:52:58.980 | INFO     | __main__:train:172 - Step 6000, Loss: 0.6780
2026-02-03 14:53:02.128 | INFO     | __main__:train:176 - Epoch 4 completed. Average loss: 0.6704
2026-02-03 14:53:02.141 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch4.pt
2026-02-03 14:53:02.628 | INFO     | __main__:train:172 - Step 6100, Loss: 0.5238
2026-02-03 14:53:06.021 | INFO     | __main__:train:172 - Step 6200, Loss: 0.5259
2026-02-03 14:53:10.154 | INFO     | __main__:train:172 - Step 6300, Loss: 0.5208
2026-02-03 14:53:14.112 | INFO     | __main__:train:172 - Step 6400, Loss: 0.5132
2026-02-03 14:53:17.898 | INFO     | __main__:train:172 - Step 6500, Loss: 0.5102
2026-02-03 14:53:21.786 | INFO     | __main__:train:172 - Step 6600, Loss: 0.5060
2026-02-03 14:53:25.427 | INFO     | __main__:train:172 - Step 6700, Loss: 0.5018
2026-02-03 14:53:28.706 | INFO     | __main__:train:172 - Step 6800, Loss: 0.4967
2026-02-03 14:53:31.986 | INFO     | __main__:train:172 - Step 6900, Loss: 0.4918
2026-02-03 14:53:35.283 | INFO     | __main__:train:172 - Step 7000, Loss: 0.4870
2026-02-03 14:53:38.585 | INFO     | __main__:train:172 - Step 7100, Loss: 0.4827
2026-02-03 14:53:42.149 | INFO     | __main__:train:172 - Step 7200, Loss: 0.4785
2026-02-03 14:53:45.769 | INFO     | __main__:train:172 - Step 7300, Loss: 0.4750
2026-02-03 14:53:49.273 | INFO     | __main__:train:172 - Step 7400, Loss: 0.4711
2026-02-03 14:53:52.873 | INFO     | __main__:train:172 - Step 7500, Loss: 0.4674
2026-02-03 14:53:56.604 | INFO     | __main__:train:172 - Step 7600, Loss: 0.4637
2026-02-03 14:53:56.928 | INFO     | __main__:train:176 - Epoch 5 completed. Average loss: 0.4635
2026-02-03 14:53:56.941 | INFO     | __main__:train:197 - Checkpoint saved: checkpoints/model_epoch5.pt
2026-02-03 14:53:56.941 | INFO     | __main__:train:199 - Training completed!
